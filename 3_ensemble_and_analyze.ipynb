{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Analyze Climate Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 3/X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In notebook 1/X we created configuration files for both NeuralHydrology's (NH) Long Short-Term Memory model and the SACSMA-SNOW17 (SAC-SMA) model. In notebook 2/X we created, and potentially ran, the SAC-SMA slurm scripts. In notebook 3/X we will manually ensemble and subsequently analyze one set of climate experiments, assuming they have been trained and tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of this notebook include 1) USGS observed streamflow data DataFrame (sourced from CAMELS), 2) dictionary of experiments and their simulated runoff data, 3) dictionary of ensembled mean hydrographs, 4) dictionary of basin metrics, and 5) a dictionary containing cdfs plotting data for ease in the next analysis notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically reload modules; ensures most recent versions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python libraries\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import xarray\n",
    "import logging\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from functions import * #load the million neuralhydrology functions I use here\n",
    "from pathlib import Path\n",
    "from ruamel.yaml import YAML\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from xarray.core.dataarray import DataArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only section you will need to edit code in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most Important Experiment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the two previous notebooks, we need to first define which experiment set we want to ensemble and analyze. If the experiment set has been ensembled and analyzed before, the existing product files can be loaded in without having to go through the process all over again. Alternatively, the 'overwrite' variable allows the notebook to overwrite those existing files and create new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Specify which model we are ensembling and analyzing; 'nh' or 'sacsma'\n",
    "model = 'nh'\n",
    "\n",
    "#Specify climate index input type; 'static' or 'dynamic'\n",
    "inputs = 'static'\n",
    "\n",
    "#Specify experiment type; 'extreme' or 'random'\n",
    "exp_type = 'extreme'\n",
    "\n",
    "#Specify forcing source\n",
    "forcing = 'daymet'\n",
    "\n",
    "#Specify if all years or only years also avaliable for the National Water Model (NWM) are used\n",
    "years = 'nwm'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notebook Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overwrite variable allows this notebook to overwrite pre-existing files created by this notebook.\n",
    "\n",
    "The 'plot_all_metrics' variable allows you to plot _all_ the avaliable metrics (listed below) or ONLY the Nash-Sutcliffe Efficiency Coefficient (NSE). All metrics are calculated regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#If files already exist, should they be overwritten by this notebook?\n",
    "overwrite = False\n",
    "\n",
    "#Do we want to plot all metrics or just the NSE?\n",
    "plot_all_metrics = False\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our paths. New path variables include nh_dir (path to neuralhydrology directory), nh_runs_dir (path to the NH runs folder), sacsma_run_dir (path to SAC-SMA results directory), camels_forcing_dir (path to the CAMELS directory), env_saves_dir (path to notebook_env_saves directory). I have specified what the general path end should look like above each line to help with this.\n",
    "\n",
    "Additionally, the NWM run (nwm_streamflow) and CAMELS identification scheme (nwm_camels_ids) is sourced from Jonathan Frame's work (https://github.com/jmframe/nwm-post-processing-with-lstm) with the National Water Center in Tuscaloosa, Alabama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "# Path to working directory (current directory)\n",
    "working_dir = Path(os.getcwd())\n",
    "\n",
    "#Path to main CAMELS directory (../camels)\n",
    "camels_dir = working_dir / 'camels'\n",
    "\n",
    "#Path to NH codebase (../neuralhydrology/neuralhydrology)\n",
    "nh_dir = working_dir / 'neuralhydrology' / 'neuralhydrology'\n",
    "\n",
    "#Path to CAMELS forcing directory (../basin_dataset_public_v1p2)\n",
    "camels_forcing_dir = camels_dir / 'basin_dataset_public_v1p2'\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#If you use this repository's directory structure, you not need to edit the paths below, but just in case...\n",
    "\n",
    "#Path to sacsma results(../sacsma/results)\n",
    "sacsma_runs_dir = working_dir / 'sacsma' / 'results'\n",
    "\n",
    "# Path to notebook_env_saves; this is where the resulting files from this notebook will be written\n",
    "env_saves_dir = working_dir / 'notebook_env_saves'\n",
    "\n",
    "#Path to NH runs (../runs/nh)\n",
    "nh_runs_dir = working_dir / 'nh_lstm' / 'runs' / 'nh'\n",
    "\n",
    "#Path to extreme dates directory (../train_test_splits)\n",
    "dates_dir = working_dir / 'train_test_splits'\n",
    "\n",
    "#Path to NWM run (../nwm_daily.pkl)\n",
    "nwm_streamflow = working_dir / 'nwm_daily.pkl'\n",
    "\n",
    "#Path to NWM CAMELS identification file\n",
    "nwm_camels_ids = working_dir / 'camels_id.npy'\n",
    "\n",
    "#Path to CAMELS attributes, version 2 file\n",
    "camels_attributes_dir  = camels_dir / 'camels_attributes_v2.0/camels_attributes_v2.0.csv'\n",
    "\n",
    "#Path to model's original experiment config files (to be able to access both nh and sacsma config info)\n",
    "configs_path = working_dir / f'nh_lstm/configs/run_configs/{model}/{inputs}/{exp_type}/{forcing}/{years}'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should not have to edit anything below this cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below orients the notebook based on the parameters specifed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Point to NeuralHydrology codebase\n",
    "sys.path.append(str(nh_dir))\n",
    "\n",
    "#Import NeuralHydrology functions\n",
    "from utils import config\n",
    "from datautils.utils import load_basin_file\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#Navigate to run directory based on model\n",
    "if model == 'nh':\n",
    "    \n",
    "    exp_runs_dir = nh_runs_dir / inputs / exp_type / forcing / years \n",
    "    \n",
    "if model == 'sacsma':\n",
    "    \n",
    "    exp_runs_dir = sacsma_runs_dir\n",
    "    \n",
    "    #Automatically set inputs to dynamic for SAC-SMA\n",
    "    inputs = 'dynamic'\n",
    "    \n",
    "#########################################################################################\n",
    "\n",
    "#Define index according to years used ('all' or 'nwm')\n",
    "if years == 'nwm':\n",
    "    \n",
    "    #Index limited to years also avaliable for NWM\n",
    "    index = pd.date_range('1995-10-01','2014-12-30')\n",
    "    \n",
    "if years == 'all':\n",
    "    \n",
    "    #All avaliable forcing years\n",
    "    index = pd.date_range('1980-10-01','2014-12-30')\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#Define avaliable metric functions and extract the names of those functions\n",
    "metrics = [alpha_nse,beta_kge,beta_nse,kge,mse,nse,rmse,pearsonr]\n",
    "\n",
    "#Get names of avaliable metric functions\n",
    "metrics_names = [x.__name__ for x in metrics]\n",
    "\n",
    "#Define metrics to plot and associated axes\n",
    "if plot_all_metrics:\n",
    "    \n",
    "    #Make dictionary ###\n",
    "    plot_metrics = metrics_names\n",
    "    axes = [(0,2,0,1),(0,3,0,1),(-1,1,0,1),(-20,1,0,1),(0,35,0,1),(-1,1,0,1),(0,5,0,1),(-0,1,0,1)]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    plot_metrics = ['nse']\n",
    "    axes = [(-1,1,0,1)]\n",
    "    \n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to create a list of the existing files in the env_saves_dir so we can check whether or not a file exists already of not. We explicitly reference the cdfs dictionary file, as it is the last output file from this notebook and if that exists, then we have already saved the previous output files as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mExperiments previously ensembled and analyzed.\n"
     ]
    }
   ],
   "source": [
    "#Create current list of files in env_saves_dir\n",
    "env_saves = os.listdir(env_saves_dir)\n",
    "\n",
    "#Explictly state if experiments were already ensembled and analyzed\n",
    "if f'cdfs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl' in env_saves:\n",
    "     print('\\033[91m'+'\\033[1m'+'Experiments previously ensembled and analyzed.')\n",
    "          \n",
    "#Warn if files will be overwritten (if overwrite == True)\n",
    "if overwrite:\n",
    "     print('\\033[91m'+'\\033[1m'+'Ensemble and analysis files will be overwritten.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of referring to the experiment run directory (exp_runs_dir), we refer to the original configuration files to get run names, config_nums, and experiment names. This is due to the differing output types between NH (bulk run directories) and SAC-SMA (directory for every basin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths to configuration files in run dir using glob\n",
    "config_paths = list(configs_path.glob(f'**/*_{forcing}_{years}_*'))\n",
    "\n",
    "#List of runs (from from run dir) from config_paths\n",
    "runs = [str(x).split('/')[-1] for x in config_paths]\n",
    "\n",
    "#List of random seeds from end of run name\n",
    "config_nums = list(np.sort(list(set([x.split(f'_{years}_')[-1].split('_')[-1].split('.')[0] for x in runs]))))\n",
    "\n",
    "#Using an example config number, get all files containing that config number; get one of every experiment\n",
    "experiments = list(np.sort(list(set([x.split(f'_{years}_')[-1].split(f'_{config_nums[0]}')[0] for x in runs if \n",
    "                                    f'_{config_nums[0]}' in x]))))         \n",
    "\n",
    "#Make sure experiments are in string format\n",
    "experiments = [str(x) for x in experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in a dummy configuration file and check which basin list was used for these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 531 basins.\n"
     ]
    }
   ],
   "source": [
    "#Path to dummy experiment file\n",
    "dummy_config_file = config_paths[0]\n",
    "\n",
    "#Read dummy_config_file\n",
    "cfg = config.Config(dummy_config_file)\n",
    "\n",
    "#Get list of basins from cfg file\n",
    "basins = load_basin_file(cfg.train_basin_file)\n",
    "\n",
    "#Make basin list where basin ID is an integer\n",
    "basins_int = [int(basin) for basin in basins]\n",
    "\n",
    "#Print how many basins are in the basins list\n",
    "print(f'There are {len(basins)} basins.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load USGS streamflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in USGS streamflow data from the CAMELS dataset to use as our observed runoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the related USGS runoff data has not already been saved into notebook_env_saves or if we want to overwrite it...\n",
    "if not f'usgs_runoff_data.pkl' in env_saves or overwrite == True:\n",
    "    \n",
    "    #Create dataframe for usgs data\n",
    "    usgs = []\n",
    "\n",
    "    #For every basin...\n",
    "    for basin in tqdm(basins):\n",
    "        \n",
    "        #Use load_forcings function to get basin area\n",
    "        _, area = load_forcings(data_dir=camels_forcing_dir,basin=basin,forcings=f'{forcing}')\n",
    "        \n",
    "        #Create throwaway dataframe for a basin using the load_usgs function\n",
    "        srs = load_usgs(data_dir=camels_forcing_dir,basin=basin, area=area)\n",
    "        \n",
    "        srs.rename(basin, inplace=True)\n",
    "        \n",
    "        #Fill columns with daily usgs data for a basin\n",
    "        usgs.append(srs)\n",
    "\n",
    "    #Make sure usgs data only for specified years\n",
    "    usgs = pd.concat(usgs,axis=1)\n",
    "    \n",
    "    #Save usgs data\n",
    "    with open(env_saves_dir / f'usgs_runoff_data.pkl', 'wb') as f:\n",
    "        pkl.dump(usgs, f)\n",
    "\n",
    "#If the file already exists and we do not want to overwrite...\n",
    "else:\n",
    "    \n",
    "    #Load usgs data\n",
    "    with open(env_saves_dir / f'usgs_runoff_data.pkl', 'rb') as f:\n",
    "        usgs = pkl.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load and save our simulated runoff values for each climate variable experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the experiment dictionary does not already exist or if we want to overwrite it...\n",
    "if not f'exps_dict_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "\n",
    "    #Initialize dataframe to hold experiments\n",
    "    exps_dict = {basin: [] for basin in basins}\n",
    "\n",
    "    #Load data into dataframes\n",
    "    print('Loading experiments...')\n",
    "\n",
    "    #For every experiment...\n",
    "    for experiment in tqdm(experiments):\n",
    "\n",
    "        #Get a list of files for an experiment from the run dir\n",
    "        files = [x for x in os.listdir(exp_runs_dir) if not x.startswith('.') and f'{experiment}' in x]\n",
    "\n",
    "        #And for every file, pair it with a configuration number.\n",
    "        for file,conf in zip(files,config_nums):\n",
    "\n",
    "            if model == 'nh':\n",
    "\n",
    "                #Define file path to test_results in a run direction\n",
    "                file_path = Path(f'{exp_runs_dir}/{file}/test/model_epoch030/test_results.p')\n",
    "\n",
    "                #Open the test_results file\n",
    "                with open (file_path, 'rb') as f:\n",
    "                    data = pkl.load(f)\n",
    "\n",
    "                #And for every basin...\n",
    "                for basin in basins:\n",
    "\n",
    "                    #Create multilevel dataframe column for an experiment, configuration pair\n",
    "                    #And fill in the columns with the simulated values from each ensemble member\n",
    "                    exps_dict[basin].append(data[basin]['1D']['xr']['QObs(mm/d)_sim'].to_pandas()[0].rename((experiment,conf)))\n",
    "\n",
    "            if model == 'sacsma':\n",
    "                \n",
    "                #For every basin...\n",
    "                for basin in basins: \n",
    "\n",
    "                    #Define file path to a basin's directory\n",
    "                    file_path = exp_runs_dir / f'{file}' / f'{basin}.pkl'\n",
    "\n",
    "                    #Open basin file\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pkl.load(f)\n",
    "\n",
    "                    #Create multilevel dataframe column for an experiment, configuration pair\n",
    "                    exps_dict[basin].append(data[1].rename((experiment,conf)))\n",
    "\n",
    "    for basin in basins:\n",
    "\n",
    "        exps_dict[basin] = pd.concat(exps_dict[basin],axis=1)\n",
    "\n",
    "    #Save dictionary of predictions for experiments\n",
    "    with open(env_saves_dir / f'exps_dict_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'wb') as f:\n",
    "        pkl.dump(exps_dict, f)\n",
    "\n",
    "#If the file already exists and we don't want to overwrite it...\n",
    "else:\n",
    "\n",
    "    #Load dictionary of predictions for experiments\n",
    "    with open(env_saves_dir / f'exps_dict_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'rb') as f:\n",
    "        exps_dict = pkl.load(f)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test start/end dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the per basin test date files so we can calculate metrics only for those years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dictionary for test dates\n",
    "test_dates_dict_og = {}\n",
    "\n",
    "#For every experiment...\n",
    "for experiment in experiments:\n",
    "        \n",
    "    #Throwaway variable of path to that file\n",
    "    x = Path(dates_dir,f'{model}_{exp_type}_test_{years}_{experiment}.pkl')\n",
    "\n",
    "    #Load test_dates for each experiment\n",
    "    with open(x,'rb') as f:\n",
    "        test_dates_dict_og[experiment] = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the model is nh...\n",
    "if model == 'nh':\n",
    "    \n",
    "    #The original test_dates_dict matches the expected nh structure\n",
    "    test_dates_dict = test_dates_dict_og\n",
    "\n",
    "#If the model is sacsma...\n",
    "if model == 'sacsma':\n",
    "    \n",
    "    #We need to make a new dictionary that switches the order of the dictionary\n",
    "    test_dates_dict = {}\n",
    "\n",
    "    #So for every experiment...\n",
    "    for experiment in experiments:\n",
    "        \n",
    "        #Create an experiment level\n",
    "        test_dates_dict[experiment] = {}\n",
    "\n",
    "        #And for every basin...\n",
    "        for basin in basins:\n",
    "            \n",
    "            #Create a level for basins\n",
    "            test_dates_dict[experiment][basin] = {}\n",
    "            \n",
    "            #Switch basin / dates order and save into new dictionary\n",
    "            test_dates_dict[experiment][basin]['start_dates'] = test_dates_dict_og[experiment]['start_dates'][basin]\n",
    "            test_dates_dict[experiment][basin]['end_dates'] = test_dates_dict_og[experiment]['end_dates'][basin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create test year ranges from start/end dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the start and end dates for the test years, we want to create a date range from those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dictionary containing date ranges sourced from start/end date values from test_dates_dict\n",
    "test_years_dict = {}\n",
    "\n",
    "#For every experiment...\n",
    "for experiment in experiments:\n",
    "    \n",
    "    #Initialize experiment level\n",
    "    test_years_dict[experiment] = {}\n",
    "    \n",
    "    #For every basin...\n",
    "    for basin in test_dates_dict[experiment].keys():\n",
    "        \n",
    "        #Initialize throwaway list to contain date ranges\n",
    "        all_dates = []\n",
    "        \n",
    "        #Define which experiment/basin we are looking at\n",
    "        loc_in_dict = test_dates_dict[experiment][basin]\n",
    "        \n",
    "        #For every start/end date pair...\n",
    "        for start,end in zip(loc_in_dict['start_dates'],loc_in_dict['end_dates']):\n",
    "            \n",
    "            #Create date range starting with start date and ending with end date\n",
    "            dates = pd.date_range(start,end,freq='D')\n",
    "            #Append those to all_dates\n",
    "            all_dates.append(dates)\n",
    "            \n",
    "        #Save all of those dates to test_years_dict\n",
    "        test_years_dict[experiment][basin] = pd.DatetimeIndex(np.unique(np.hstack(all_dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dates_dict['p_mean_dyn_low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dates_dict['p_mean_dyn_high']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Experiment Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded all the data necessary to begin ensembling our runs, so now... Let's ensemble our runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create mean hydrographs from predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to ensemble the ensemble members for each experiment. This allows us to create one mean representative hydrograph to calculate metrics on. This reduces the effect of randomness and gives us a better understanding of the model's overall predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file does not already exist or we want to overwrite it...\n",
    "if not f'mean_hydrographs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "    \n",
    "    #Calculate ensemble results for experiments\n",
    "    print('Calculating ensemble results...')\n",
    "    \n",
    "    #Initialize dictionary for mean hydrographs\n",
    "    mean_hydrographs = {basin: [] for basin in basins}\n",
    "    \n",
    "    #For every basin...\n",
    "    for basin in tqdm(basins):\n",
    "        \n",
    "        #For every experiment...\n",
    "        for experiment in experiments:\n",
    "            \n",
    "            #Append averaged experiment series list into dictionary\n",
    "            mean_hydrographs[basin].append(exps_dict[basin][experiment].mean(axis=1).rename(experiment))\n",
    "            \n",
    "    for basin in basins:\n",
    "\n",
    "        mean_hydrographs[basin] = pd.concat(mean_hydrographs[basin],axis=1)\n",
    "    \n",
    "    #Save mean_hydrographs\n",
    "    with open(env_saves_dir / f'mean_hydrographs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'wb') as f:\n",
    "        pkl.dump(mean_hydrographs, f)\n",
    "    \n",
    "    print('Ensemble results saved.')\n",
    "\n",
    "#If the file already exists and we do not want to overwrite it...\n",
    "else:\n",
    "    \n",
    "    #Load mean_hydrographs\n",
    "    with open(env_saves_dir / f'mean_hydrographs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'rb') as f:\n",
    "        mean_hydrographs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize observed USGS streamflow values of one experiment and compare simulated streamflow values from two example experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set plot title\n",
    "plt.title(f'Basin {basins[0]} Hydrograph')\n",
    "#Plot USGS observed streamflow data for an example basin\n",
    "plt.plot(usgs[basins[0]].iloc[365:-365],label='usgs')\n",
    "#Plot simulated streamflow values from one example experiment\n",
    "plt.plot(mean_hydrographs[basins[0]][experiments[0]].iloc[365:-365],label=f'{experiments[0]}')\n",
    "#Plot simulated streamflow values from another example experiment\n",
    "plt.plot(mean_hydrographs[basins[0]][experiments[1]].iloc[365:-365],label=f'{experiments[1]}')\n",
    "#Show legend\n",
    "plt.legend()\n",
    "#Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If interactive plots enabled, plot needs to be closed explicitly\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#A closer look\n",
    "plt.title(f'Basin {basins[0]} Hydrograph')\n",
    "plt.plot(usgs[basins[0]].loc['2000'],label='usgs')\n",
    "plt.plot(mean_hydrographs[basins[0]][experiments[0]].loc['2000'],label=f'{experiments[0]}')\n",
    "plt.plot(mean_hydrographs[basins[0]][experiments[1]].loc['2000'],label=f'{experiments[1]}')\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Streamflow (mm/day)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate metrics from observed and simulated streamflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mean hydrographs calculated/loaded above, we will use our test date ranges to create masks as to only run metric calculations on the specified test years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If this file does not already exist or if we want to overwrite it...\n",
    "if not f'basin_metrics_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "    \n",
    "    #Initialize metrics_dict\n",
    "    metrics_dict = {}\n",
    "\n",
    "    #For every experiment...\n",
    "    for experiment in experiments:\n",
    "        \n",
    "        #Create dataframe with basin index and metric columns\n",
    "        print(f'Calculating metrics for {experiment}...')\n",
    "        metrics_dict[experiment] = pd.DataFrame(index=basins,columns=metrics_names)\n",
    "        \n",
    "        #For every metric and corresponding function pair...\n",
    "        for name,func in zip(tqdm(metrics_names),metrics):\n",
    "            \n",
    "            #For every basin...\n",
    "            for basin in basins:\n",
    "                \n",
    "                #Create a list of test dates stacked into one datetime index\n",
    "                mask = test_years_dict[experiment][basin]\n",
    "                #Get observed values for the dates in the mask\n",
    "                obs = usgs[basin].loc[mask]\n",
    "                #Get simualted values for the dates in the mask\n",
    "                sim = mean_hydrographs[basin][experiment].loc[mask].clip(0)\n",
    "                \n",
    "                #Calculate metrics between observed and simulated streamflow values\n",
    "                metrics_dict[experiment][name].loc[basin] = func(obs,sim)\n",
    "    \n",
    "    #Save metrics_dict\n",
    "    with open(env_saves_dir / f'basin_metrics_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'wb') as f:\n",
    "        pkl.dump(metrics_dict, f)\n",
    "\n",
    "#If the file already exists and we don't want to overwrite it...\n",
    "else:\n",
    "\n",
    "    #Load metrics dict\n",
    "    with open(env_saves_dir / f'basin_metrics_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'rb') as f:\n",
    "        metrics_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create NWM benchmark and calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in notebook 1/X, using the NWM run as a benchmark may provide insight into the comparative predictive ability and degredation of NH and SAC-SMA models. If we specified that we only wanted to run experiments on data from years also avaliable for the NWM, this is where that benchmark materializes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in the supplemental CAMELS attributes file. This will be useful for several reasons; in this case, we need to extract basin information to convert NWM streamflow (m3/s) to mm to match model simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load CAMELS attributes file with basin id as index\n",
    "# attributes = pd.read_csv(camels_attributes_dir, sep=';', index_col='gauge_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if years == 'nwm':\n",
    "    \n",
    "#     #If we are ensembling a random run (the NWM was not trained on extreme values, more closely related to random runs)...\n",
    "#     if exp_type == 'random':\n",
    "\n",
    "#         #If the file does not already exist or if we want to overwrite it...\n",
    "#         if not f'nwm_random_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "\n",
    "#             #Load the streamflow simulations from the nwm run\n",
    "#             with open(nwm_streamflow,'rb') as f:\n",
    "#                 nwm = pkl.load(f)\n",
    "\n",
    "#             #Load the NWM identification key (basins labeled differently between CAMELS and NWM)\n",
    "#             with open(nwm_camels_ids,'rb') as f:\n",
    "#                 camels_nwm_basins = np.load(f)\n",
    "\n",
    "#             #Create a dataframe\n",
    "#             keys_basins_dict = pd.DataFrame()\n",
    "#             #Load the nwm basin keys\n",
    "#             keys_basins_dict['nwm_keys'] = nwm.keys()\n",
    "#             #And the camels ids so that we can relate the two them\n",
    "#             keys_basins_dict['camels_id'] = camels_nwm_basins    \n",
    "\n",
    "#             #Create a new dictionary (fixed refers to relating the camels ids to the nwm keys)\n",
    "#             nwm_fixed = {}\n",
    "\n",
    "#             #And for every nwm key, camels id pair\n",
    "#             for key,basin in zip(keys_basins_dict['nwm_keys'],keys_basins_dict['camels_id']):\n",
    "\n",
    "#                 #Load simulations for a basin into a basin's dataframe\n",
    "#                 nwm_fixed[basin] = nwm[key]\n",
    "\n",
    "#             #Retrieve area index information from attributes for conversion\n",
    "#             area_index='area_geospa_fabric'\n",
    "\n",
    "#             #Define conversion values\n",
    "#             #Seconds in a day\n",
    "#             sid = 60*60*24\n",
    "#             #Meters in a kilometer\n",
    "#             mikm = 0.001\n",
    "#             #Millimeters in a meter\n",
    "#             mmim = 0.001\n",
    "\n",
    "#             #Create dictionary to contain converted streamflow values\n",
    "#             nwm_scaled = {}\n",
    "\n",
    "#             #For every basin...\n",
    "#             for i, basin in enumerate(basins):  \n",
    "\n",
    "#                 #Create a dictionary entry for a basin\n",
    "#                 nwm_scaled[basin] = []\n",
    "#                 #Retreive area index attribute for a basin\n",
    "#                 A = attributes.loc[int(basin), area_index]\n",
    "#                 #Create conversion factor using conversion values defined above\n",
    "#                 conversion_factor = sid * mmim / A\n",
    "#                 #Multiply conversion factor by nwm simulation values and save into nwm_scaled\n",
    "#                 nwm_scaled[basin] = conversion_factor * nwm_fixed[int(basin)]['NWM_RUN']\n",
    "\n",
    "#             #And for every basin...\n",
    "#             for basin in basins:\n",
    "\n",
    "#                 #Clip the basin's dataframe based on the defined index\n",
    "#                 nwm_scaled[basin] = nwm_scaled[basin].loc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate metrics for the NWM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if years == 'nwm':\n",
    "    \n",
    "#     #If we are ensembling a random run...\n",
    "#     if exp_type == 'random':\n",
    "\n",
    "#         #And if the file does not already exist or we want to overwrite it...\n",
    "#         if not f'basin_metrics_nwm_random_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "\n",
    "#             #Initialize metrics_dict\n",
    "#             nwm_metrics_dict = {}\n",
    "\n",
    "#             print(f'Calculating NWM metrics...')\n",
    "\n",
    "#             #For every experiment...\n",
    "#             for experiment in tqdm(experiments):\n",
    "\n",
    "#                 #Create dataframe with basin index and metric columns\n",
    "#                 nwm_metrics_dict[experiment] = pd.DataFrame(index=basins,columns=metrics_names)\n",
    "\n",
    "#                 #For every metric to be calculated...\n",
    "#                 for name,func in zip(metrics_names,metrics):\n",
    "\n",
    "#                     #For every basin...\n",
    "#                     for basin in basins:\n",
    "\n",
    "#                         mask = pd.DatetimeIndex(np.unique(np.hstack(test_years_dict[experiment][basin])))\n",
    "\n",
    "#                         #Get observed values for the dates in the mask\n",
    "#                         obs = usgs[basin].loc[mask]\n",
    "#                         sim = nwm_scaled[basin].loc[mask]\n",
    "\n",
    "#                         #Calculate metrics between observed and simulated streamflow values\n",
    "#                         nwm_metrics_dict[experiment][name].loc[basin] = func(obs,sim)\n",
    "\n",
    "#             #Save metrics_dict\n",
    "#             with open(env_saves_dir / f'basin_metrics_nwm_random_{forcing}_{years}.pkl', 'wb') as f:\n",
    "#                 pkl.dump(nwm_metrics_dict, f)\n",
    "\n",
    "#         #If the file already exists and we do not want to overwrite it...\n",
    "#         else:\n",
    "\n",
    "#             #Load metrics dict\n",
    "#             with open(env_saves_dir / f'basin_metrics_nwm_random_{forcing}_{years}.pkl', 'rb') as f:\n",
    "#                 nwm_metrics_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if years == 'nwm':\n",
    "    \n",
    "#     #If we are ensembling a random run...\n",
    "#     if exp_type == 'random':\n",
    "\n",
    "#         #If the file does not already exist or we want to overwrite it...\n",
    "#         if not f'cdfs_nwm_random_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "\n",
    "#             #Initialize dictionary to store sorted metric values\n",
    "#             nwm_cdfs = {}\n",
    "\n",
    "#             #For every experiment...\n",
    "#             for experiment in experiments:\n",
    "\n",
    "#                 #Initialize experiment level\n",
    "#                 nwm_cdfs[experiment] = {}\n",
    "\n",
    "#                 #For every metric...\n",
    "#                 for metric in list(nwm_metrics_dict[experiment].keys()):\n",
    "\n",
    "#                     #Sort the metric values in increasing order\n",
    "#                     x = np.sort(nwm_metrics_dict[experiment][metric].dropna(axis=0))\n",
    "#                     #Create range based on length of x\n",
    "#                     y = np.arange(len(x))/(float(len(x)))\n",
    "#                     #Make y a list\n",
    "#                     y = y.tolist()\n",
    "\n",
    "#                     #Initialize metric levl\n",
    "#                     nwm_cdfs[experiment][metric] = {}\n",
    "\n",
    "#                     #Save x and y lists in cdfs dictionary\n",
    "#                     nwm_cdfs[experiment][metric]['x'] = x\n",
    "#                     nwm_cdfs[experiment][metric]['y'] = y\n",
    "\n",
    "#             #Save cdfs\n",
    "#             with open(env_saves_dir / f'cdfs_nwm_random_{forcing}_{years}.pkl', 'wb') as f:\n",
    "#                 pkl.dump(nwm_cdfs, f)\n",
    "\n",
    "#         #If the file already exists and we do not want to overwrite it...\n",
    "#         else:\n",
    "\n",
    "#             #Load cdfs\n",
    "#             with open(env_saves_dir / f'cdfs_nwm_random_{forcing}_{years}.pkl', 'rb') as f:\n",
    "#                 nwm_cdfs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created random benchmarks for the NWM run, let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if years == 'nwm':\n",
    "    \n",
    "#     #If we are ensembling a random run...\n",
    "#     if exp_type == 'random':\n",
    "\n",
    "#         #For every metric and associated axes ranges...\n",
    "#         for metric,ax in zip(plot_metrics,axes):\n",
    "\n",
    "#             #For every experiment...\n",
    "#             for experiment in experiments:\n",
    "\n",
    "#                 #Define x and y from the nwm_cdfs dictionary\n",
    "#                 x = nwm_cdfs[experiment][metric]['x']\n",
    "#                 y = nwm_cdfs[experiment][metric]['y']\n",
    "\n",
    "#                 #Make sure the experiment is int type\n",
    "#                 experiment = int(experiment)\n",
    "\n",
    "#                 #If the experiment is one less than the length of the amount of experiments...\n",
    "#                 if experiment == len(experiments)-1:\n",
    "\n",
    "#                     #Plot and label\n",
    "#                     plt.plot(x, y,label = f'nwm_random',c='grey',alpha=0.5,lw=0.5)\n",
    "\n",
    "#                 #If the experiment is any other value...\n",
    "#                 else:\n",
    "\n",
    "#                     #Just plot\n",
    "#                     plt.plot(x, y,c='grey',alpha=0.5,lw=0.5)\n",
    "\n",
    "#         #Add grid\n",
    "#         plt.grid()\n",
    "#         #Define axis limits\n",
    "#         plt.axis(ax)\n",
    "#         #Show legend\n",
    "#         plt.legend()\n",
    "#         #Define x label\n",
    "#         plt.xlabel(f'{metric}')\n",
    "#         #Define y label\n",
    "#         plt.ylabel('Frequency (among 531 bains)')\n",
    "\n",
    "#         #Show plot\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if years == 'nwm':\n",
    "    \n",
    "#     #If interactive plots enabled, plot needs to be explicitly closed\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Cumulative Density Function Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the plot_all_metrics value, here we will create and plot cumulative density function (cdf) plots to visualize and compare model performance across simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file does not already exist or we want to overwrite it...\n",
    "if not f'cdfs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl' in env_saves or overwrite == True:\n",
    "    \n",
    "    #Initialize dictionary to store sorted metric values\n",
    "    cdfs = {experiment: {metric: {} for metric in metrics_dict[experiment].keys()} for experiment in experiments}\n",
    "\n",
    "    #For every experiment...\n",
    "    for experiment in experiments:\n",
    "        \n",
    "        #For every metric...\n",
    "        for metric in list(metrics_dict[experiment].keys()):\n",
    "            \n",
    "            #Sort the metric values in increasing order\n",
    "            x = np.sort(metrics_dict[experiment][metric].dropna(axis=0))\n",
    "            #Create range based on length of x\n",
    "            y = np.arange(len(x))/(float(len(x)))\n",
    "            #Make y a list\n",
    "            y = y.tolist()\n",
    "\n",
    "            #Save x and y lists in cdfs dictionary\n",
    "            cdfs[experiment][metric]['x'] = x\n",
    "            cdfs[experiment][metric]['y'] = y\n",
    "    \n",
    "    #Save cdfs\n",
    "    with open(env_saves_dir / f'cdfs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'wb') as f:\n",
    "        pkl.dump(cdfs, f)\n",
    "\n",
    "#If the file already exists and we do not want to overwrite it...\n",
    "else:\n",
    "    \n",
    "    #Load cdfs\n",
    "    with open(env_saves_dir / f'cdfs_{model}_{inputs}_{exp_type}_{forcing}_{years}.pkl', 'rb') as f:\n",
    "        cdfs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell was designed specifically for the original p_mean and aridity experiments only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we ensembled extreme runs...\n",
    "if exp_type == 'extreme':\n",
    "    \n",
    "    #Initialize colors dictionary to represent experiment type\n",
    "    exp_colors = {}\n",
    "\n",
    "    #Lighter colors represent experiments testing on low climate variables and vice versa\n",
    "    exp_colors['aridity_dyn_high'] = ['r','-']\n",
    "    exp_colors['aridity_dyn_low'] = ['lightcoral','-']\n",
    "    exp_colors['p_mean_dyn_high'] = ['blue','-']\n",
    "    exp_colors['p_mean_dyn_low'] = ['cornflowerblue','-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For every metric and associated axes ranges...\n",
    "for metric,ax in zip(plot_metrics,axes):\n",
    "    \n",
    "    #For every experiment...\n",
    "    for experiment in experiments:\n",
    "        \n",
    "        #If we ensembled extreme runs...\n",
    "        if exp_type == 'extreme':\n",
    "            \n",
    "            #Define line variables according to exp_colors dict\n",
    "            c = exp_colors[experiment][0]\n",
    "            l = exp_colors[experiment][1]\n",
    "            \n",
    "            #Define x,y values from cdfs_dict\n",
    "            x = cdfs[experiment][metric]['x']\n",
    "            y = cdfs[experiment][metric]['y']\n",
    "            \n",
    "            #Plot and label x and y values\n",
    "            plt.plot(x, y, l,label = f'{experiment}',c=c)\n",
    "            \n",
    "        #If we ensembled random runs...\n",
    "        if exp_type == 'random':\n",
    "            \n",
    "            #Define x,y values from cds_dict\n",
    "            x = cdfs[str(experiment)][metric]['x']\n",
    "            y = cdfs[str(experiment)][metric]['y']\n",
    "            \n",
    "            #Plot and label x and y values\n",
    "            plt.plot(x, y, label = f'{experiment}')\n",
    "    \n",
    "    #Add grid\n",
    "    plt.grid()\n",
    "    #Define axis limits\n",
    "    plt.axis(ax)\n",
    "    #Show legend\n",
    "    plt.legend()\n",
    "    #Define x label\n",
    "    plt.xlabel(f'{metric}')\n",
    "    #Define y label\n",
    "    plt.ylabel('Frequency (among 531 bains)')\n",
    "    #Define plot title\n",
    "    plt.title(f'{model}_{inputs}_{exp_type}_{forcing}_{years}')\n",
    "\n",
    "    #Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If interactive plots avaliable, explicitly close plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Plots For Extreme Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics may be correlated with basin location, lat./lon., etc. so here we plot the metrics spatially to understand the geographic distribution and variance of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot metrics spatially for one experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attributes reloaded just in case we want to rerun \n",
    "attributes = pd.read_csv(camels_attributes_dir, sep=';', index_col='gauge_id')\n",
    "\n",
    "#If we are ensembling an extreme experiment...\n",
    "if exp_type == 'extreme':\n",
    "\n",
    "    #Drop attribute columns that are not quantitative values \n",
    "    #(could potentially be included if numerically categorized)\n",
    "    drop_cols = ['high_prec_timing','low_prec_timing','geol_1st_class',\n",
    "                 'geol_2nd_class','dom_land_cover','water_frac','organic_frac']\n",
    "\n",
    "    #Drop drop_cols\n",
    "    attributes = attributes.drop(drop_cols, axis=1)\n",
    "    #Drop any columns that are not in the basins_int list (just in case)\n",
    "    attributes = attributes.loc[basins_int]\n",
    "\n",
    "    #Initiate lists of basin latitudes and longitudes\n",
    "    plot_lats = []\n",
    "    plot_lons = []\n",
    "\n",
    "    #For every basin...\n",
    "    for basin in basins:\n",
    "        \n",
    "        #Append lat/long from attribute columns to lists\n",
    "        plot_lats.append(attributes.loc[int(basin),'gauge_lat'])\n",
    "        plot_lons.append(attributes.loc[int(basin),'gauge_lon'])\n",
    "\n",
    "    #Make list arrays\n",
    "    plot_lats = np.array(plot_lats)\n",
    "    plot_lons = np.array(plot_lons)\n",
    "\n",
    "    #Variables for subplots; names do not represent anything\n",
    "    n = [0,0,1,1]\n",
    "    m = [0,1,0,1]\n",
    "\n",
    "    #For every metric and associated axes...\n",
    "    for metric,a in zip(plot_metrics,axes):\n",
    "\n",
    "        #Create a 2x2 grid of plots (one plot for each experiment)\n",
    "        fig, axs = plt.subplots(2,2,figsize = (10,6), sharex=True, sharey=True)\n",
    "\n",
    "        #For every experiment...\n",
    "        for i,experiment in enumerate(experiments):\n",
    "\n",
    "            #Create throwaway list of metrics\n",
    "            m_list = []\n",
    "\n",
    "            #For every basin...\n",
    "            for basin in basins:\n",
    "                \n",
    "                #Get metric data for a basin for that experiment\n",
    "                m_data = metrics_dict[experiment].loc[basin,metric]\n",
    "                #Append that data to m_list\n",
    "                m_list.append(m_data)\n",
    "\n",
    "            #Plot scatter plot of basin locations and color based on their metric value\n",
    "            im = axs[n[i],m[i]].scatter(plot_lons, plot_lats,\n",
    "                c=np.array(m_list),\n",
    "                s=20,\n",
    "                vmin=a[0],vmax=a[1],\n",
    "                cmap='winter')\n",
    "\n",
    "            #Return the values (min, max) that are mapped to the colormap limits\n",
    "            clims = im.get_clim()\n",
    "            #Set main plot title\n",
    "            fig.suptitle(f'{metric}', fontsize=20)\n",
    "            #Set plot titles based on location and corresponding experiment\n",
    "            axs[n[i],m[i]].set_title(f'{experiment}')\n",
    "\n",
    "\n",
    "            #Define x and y labels\n",
    "            for ax in axs.flat:\n",
    "                \n",
    "                #Set x and y labels\n",
    "                ax.set(xlabel='Lat.', ylabel='Lon.')\n",
    "\n",
    "        #Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "        for ax in axs.flat:\n",
    "            \n",
    "            #Only show x and y labels on outer plots\n",
    "            ax.label_outer()    \n",
    "\n",
    "        #Add colorbar\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        #Specify colorbar dimensions, size, and location\n",
    "        cbar_ax = fig.add_axes([1, 0.11, 0.02, 0.75])\n",
    "        #Show colorbar, specify the data we are coloring, and show it\n",
    "        fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "        #Plot with tight layout\n",
    "        plt.tight_layout()\n",
    "        #Show plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If interactive plots enabled, plot needs to be explicitly closed\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot metrics difference of two extreme experiments spatially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we are ensembling an extreme experiment...\n",
    "if exp_type == 'extreme':\n",
    "\n",
    "    #For every metric and associated axes...\n",
    "    for metric,a in zip(plot_metrics,axes):\n",
    "        \n",
    "        #Initalize plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "\n",
    "        #Create throwaway list of metrics\n",
    "        m_list = []\n",
    "\n",
    "        #For every basin...\n",
    "        for basin in basins:\n",
    "            #Get metric data for a basin for that experiment\n",
    "            m_data = metrics_dict[experiments[1]].loc[basin,metric] - metrics_dict[experiments[0]].loc[basin,metric]\n",
    "            #Append that data to m_list\n",
    "            m_list.append(m_data)\n",
    "\n",
    "        #Plot scatter plot of basin locations and color based on their metric value\n",
    "        im = plt.scatter(plot_lons, plot_lats,\n",
    "            c=np.array(m_list),\n",
    "            s=20,\n",
    "            vmin=-0.5,vmax=0.5,\n",
    "            cmap='RdBu')\n",
    "\n",
    "        #Return the values (min, max) that are mapped to the colormap limits\n",
    "#         clims = im.get_clim()\n",
    "        plt.title(f'{experiments[1]} - {experiments[0]}')\n",
    "        plt.suptitle(f'{metric}', fontsize=15)\n",
    "        plt.colorbar(im)\n",
    "        plt.xlabel('Lat.')\n",
    "        plt.ylabel('Lon.')\n",
    "\n",
    "    #Plot\n",
    "    plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If interactive plots enabled, plot needs to be explicitly closed\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have ensembled a model's runs, calculated its metrics, and took a look at differences performance under different climate distribution changes, we can use the output files from this notebook to compare two different sets of experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
