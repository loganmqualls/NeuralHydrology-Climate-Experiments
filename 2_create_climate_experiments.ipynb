{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Extreme Year Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 2/X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written by Logan Qualls. Data for this work is sourced from the National Center for Atmospheric Research's Catchment Attributes and Meterology for Large-Sample Studies (CAMELS) dataset, and this notebook is designed to work specifically with Frederik Kratzert's NeuralHydrology (NH; https://github.com/neuralhydrology/neuralhydrology) and Grey Nearing's SACSMA-SNOW17 (SAC-SMA; https://github.com/Upstream-Tech/SACSMA-SNOW17). NH provides a flexible framework with a variety of tools specifically designed for straightforward application of Long Short-Term Memory networks to hydrological modeling. The SACSMA-SNOW17 model provides a Python interface for the SAC-SMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dynamic climate indices files we created in notebook 1/X, we can now create model configuration files. This notebook outputs 1) pickled dictionaries of per-basin train and test sets and 2) configuration files for both NH and SAC-SMA.\n",
    "\n",
    "NH and SAC-SMA use separate train and test set and configuration files due to conflicting input formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically reload modules; ensures most recent versions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python libraries\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import NeuralHydrology functions\n",
    "from supplemental import config\n",
    "from supplemental.utils import load_basin_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most Important Experiment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define which models we want to create configuration files for. The main components include 1) which models we want to create config files for, 2) what type of climate inputs we want to include in the model, 3) type of experiment, defined by the nature of the train and test splits, 4) the forcing source, and 5) the range of years the train and test years are chosen from.\n",
    "\n",
    "This notebook can create configuration files for both NH and SAC-SMA simultaneously, as well as individually. This can be decided through the set Boolean variables, \"nh\" and \"sacsma\". The \"inputs\" variable refers to the nature of the climate inputs to be included in the NH LSTM model. To include the static CAMELS climate attributes values (Addor et al., 2017), inputs should be set to \"static\". To include the updating dynamic climate indices we created in notebook 1/X, inputs should be set to \"dynamic\". The experiment type variable, \"exp_type\", refers to the nature of the train test sets to be used in the model. These can either be \"extreme\", meaning the train and test sets are composed of years characterized by an extreme climate value, or \"random, meaning the train and test sets are composed of years randomly chosen. The \"forcing\" variable defines the forcing source of the climate inputs. The forcing sources avaliable through the CAMELS dataset include \"daymet\", \"maurer\", \"maurer_extended\", \"nldas\", and \"nldas_extended\". Lastly, the \"years\" variable determines the range of years the train and test sets can pull from. The options are \"all\", referring to all years avaliable through CAMELS (1980-2014), and \"nwm\", refering to the years also avaliable for the National Water Model (NWM; 1995-2014). This option is included so that the NWM can be used as a benchmark model to the LSTM and SAC-SMA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Create SAC-SMA configuration files? \n",
    "sacsma = False\n",
    "#Create NH configuration files?\n",
    "nh = True\n",
    "\n",
    "#Climate index input type: 'static' or 'dynamic'\n",
    "# NOTE: SAC-SMA DOES NOT ACCEPT STATIC OR DYNAMIC CLIMATE INPUTS\n",
    "inputs = 'static'\n",
    "\n",
    "#Experiment type: train/test on 'extreme' or 'random' years\n",
    "exp_type = 'extreme'\n",
    "\n",
    "#Forcing source: CAMELS forcing data source? (daymet', 'nldas'(_extended'), 'maurer'(_extended'))\n",
    "forcing = 'nldas_extended'\n",
    "\n",
    "#Range of years train/test sets sourced from: 'all' or 'nwm'\n",
    "years = 'nwm'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Experiment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining parameters are less apt to change. \n",
    "\n",
    "First we need to define which dynamic climate indices to create train and test sets and configuration files for. Default is p_mean_dyn and aridity_dyn. This list must consist of the climate indices names in the dynamic climate indices file. If we are creating random experiments, we want to define how many random train and test sets we want to create. These are generally consistent throughout all experiments, meaning only one set of 10 random train test tests will be created, to act as a consistent, unbiased benchmark for the extreme experiments. We also need to specify how many years we want the train and test sets to consist of. Default for both is 5. Lastly, we need to specify the number of ensemble members we want to create (num_seeds; default 10) and which seed we would like to start on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#If exp_type == 'extreme': Which indexes do we want to create extreme train/test sets for\n",
    "use_climate_indexes = ['p_mean_dyn', 'aridity_dyn']\n",
    "\n",
    "#If exp_type == 'random': How many random train/test sets do we want to create\n",
    "num_random_experiments = 10\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#Specify number of years to be used in train and test sets\n",
    "n_train_years = 5\n",
    "n_test_years = 5\n",
    "\n",
    "#Specify number of ensemble members and first random seed value (increases by 1 for n num_seeds)\n",
    "num_seeds = 10\n",
    "first_seed = 100\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notebook Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a safety measure, variables to control file overwriting are included. If the files already exist, we have to explicitly, and separately, define if we want to overwrite the configuration and/or train/test (dates) files. If we are creating configuration files for a set of random experiments, we need to be careful about overwriting the dates files to maintain a consitent benchmark for the extreme models. Also, if we want the resulting plots to be interactive, set interactive_plots to True; if we want plots to be inline, set it to False. For this notebook, it is not additive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Overwrite existing configuration files?\n",
    "overwrite_configs = False\n",
    "\n",
    "#Overwrite existing train/test date files? TYPICALLY FALSE FOR RANDOM EXPERIMENTS\n",
    "overwrite_dates = False\n",
    "\n",
    "#Define if in-notebook plots are interactive (typically not needed)\n",
    "interactive_plots = False\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the paths to different directories and files. If the native repository and naming structure is being used, it is unlikely that any changes will have to be made. Several of the filepaths and names are dynamically determined by the experimental variables we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Path to working directory (where this notebook is)\n",
    "working_dir = Path(os.getcwd())\n",
    "\n",
    "#Path to \"configs\" directory (../configs)\n",
    "config_dir = working_dir / 'configs'\n",
    "\n",
    "#Path to folder to save train/test sets to\n",
    "dates_dir = config_dir / 'train_test_sets'\n",
    "\n",
    "#Path to NeuralHydrology codebase ('../neuralhydrology/neuralhydrology')\n",
    "nh_dir = working_dir / 'neuralhydrology' / 'neuralhydrology'\n",
    "\n",
    "#Dynamic path to save the NH configuration files to (based on defined variable values)\n",
    "nh_configs_path = working_dir / f'nh/configs/{inputs}/{exp_type}/{forcing}/{years}'\n",
    "\n",
    "#Dynamic path to save the SAC-SMA configuration files to (based on defined variable values)\n",
    "sacsma_configs_path = working_dir / f'sacsma/configs/{inputs}/{exp_type}/{forcing}/{years}'\n",
    "\n",
    "#Path to dummy config file (based on defined input type)\n",
    "dummy_config_file = config_dir / 'dummy_configs' / f'climate_experiment_{inputs}_dummy.yml' \n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are creating configuration for extreme experiments (exp_type = 'extreme), we need to specify which dynamic climate indices file we want to use to create the train and test sets and also be used in the resulting experiment configuation files. Below we define the desired window, load in the basin list and get the length of that list, and retrieve a dynamic climate indices file based on that information plus the defined value of the forcing variable. The default basin list is the 531_basin_list.txt which uses 531, out of 671, CAMELS basins. The remaining basins were excluded for a variety of reasons, discussed in Newman et al., 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Define window length used in the desired dynamic climate indices file\n",
    "window = 365\n",
    "\n",
    "#Load in dummy configuration file\n",
    "cfg = config.Config(dummy_config_file)\n",
    "\n",
    "#Extract file of list of basin from the dummy config\n",
    "basins = load_basin_file(cfg.train_basin_file)\n",
    "\n",
    "#Get length of basin list in dummy config\n",
    "num_basins = len(basins)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#Filepath and name to (load or save a) dynamic climate indexes pickle file\n",
    "climate_indices_file = config_dir / f'dynamic_climate_indices/dyn_clim_indices_{forcing}_{num_basins}basins_{str(window)}.p'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should not have to edit anything below this cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"holdout_types\" can be a confusing naming scheme (@gsnearing). Holdout types are the \"type\" of years that will be excluded, or _held out_, of the **training** set and ultimately used in the **test** set. Note that these _only_ need to be defined for extreme experiments.\n",
    "\n",
    "For example, if we want to simulate an increasingly arid climate, represented by the dynamic mean precipitation index (p_mean), we want to **train on high** average p_mean years and **test on low** average p_mean years; this experiment would be called **p_mean_dyn_low**, because years with **low** average p_mean years are excluded from the training set and used in the **test** set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below orients the notebook based on the parameters specifed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Explicitly describe types of extremes\n",
    "holdout_types = ['low', 'high']\n",
    "\n",
    "#List the files currently in the dates_dir (for reference)\n",
    "date_files = os.listdir(dates_dir)\n",
    "\n",
    "#Create list of seeds from seed variables\n",
    "seeds = [str(x) for x in list(range(first_seed, first_seed + num_seeds))]\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "#If we are creating random experiments...\n",
    "if exp_type == 'random':\n",
    "    \n",
    "    #Create experiment names consisting of integers (for n num_random_experiments)\n",
    "    experiments = list(range(num_random_experiments))\n",
    "\n",
    "#If we are creating extreme experiments...\n",
    "if exp_type == 'extreme':\n",
    "    \n",
    "    #Make an experiment for...\n",
    "    experiments = []\n",
    "    \n",
    "    #...each climate index and...\n",
    "    for ind in use_climate_indexes:\n",
    "        \n",
    "        #Isolate index name from \"dyn\", in case we are creating static experiments\n",
    "        ind = ind.split('_dyn')[0]\n",
    "        \n",
    "        #...holdout_type...\n",
    "        for htype in holdout_types:\n",
    "            \n",
    "            #Name the experiment according to that index and holdout type\n",
    "            experiment = f'{ind}_{htype}'\n",
    "            #Append experiment name to experiments list\n",
    "            experiments.append(experiment)\n",
    "    \n",
    "    #I like having my experiments in alphabetical order\n",
    "    experiments = list(np.sort(experiments))\n",
    "    \n",
    "#########################################################################################\n",
    "\n",
    "#If forcing == 'nldas_extended'...\n",
    "if forcing == 'nldas_extended':\n",
    "    \n",
    "    #The forcing data column names for NH (ndyns) and SAC-SMA (sdyns) are as follows\n",
    "    ndyns = ['PRCP(mm/day)','SRAD(W/m2)','Tmax(C)','Tmin(C)','Vp(Pa)']\n",
    "    sdyns = ['PRCP(mm/day)','SRAD(W/m2)','Tmax(C)','Tmin(C)','Vp(Pa)']\n",
    "    \n",
    "#If forcing == 'daymet'...\n",
    "if forcing == 'daymet' or 'maurer':\n",
    "    \n",
    "    #The forcing data column names for NH (ndyns) and SAC-SMA (sdyns) are as follows\n",
    "    ndyns = ['prcp(mm/day)','srad(W/m2)','tmax(C)','tmin(C)','vp(Pa)']\n",
    "    sdyns = ['prcp(mm/day)','srad(W/m2)','tmax(C)','tmin(C)','vp(Pa)']\n",
    "    \n",
    "#########################################################################################\n",
    "\n",
    "#Make plots interactive or inline based on interactive_plots variable\n",
    "if interactive_plots:\n",
    "    \n",
    "    %matplotlib notebook\n",
    "    %matplotlib notebook\n",
    "    \n",
    "else:\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure we understand the magnitude of running this notebook, some warnings are printed to remind you of your pre-existing experiments and current overwrite variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we are creating configurations for NH...\n",
    "if nh:\n",
    "    \n",
    "    #Check if the filepath to the configs already exists, and if it does...\n",
    "    if os.path.isdir(nh_configs_path) == True:\n",
    "        \n",
    "        #Warn us!\n",
    "        print('\\033[91m'+'\\033[1m'+'NH exeriments already exist.')\n",
    "\n",
    "#If we are creating configurations for SAC-SMA...\n",
    "if sacsma:\n",
    "    \n",
    "    #Check if the filepath to the configs already exists, and if it does...\n",
    "    if os.path.isdir(sacsma_configs_path) == True:\n",
    "        \n",
    "        #Warn us!\n",
    "        print('\\033[91m'+'\\033[1m'+'SAC-SMA exeriments already exist.')\n",
    "\n",
    "#If we said we wanted to overwrite config files, that's fine, but...\n",
    "if overwrite_configs == True:\n",
    "    \n",
    "    #Warn us!\n",
    "    print('\\nConfiguration files will be overwritten.')\n",
    "    \n",
    "#If we said we wanted to overwrite config files, that's fine, but...\n",
    "if overwrite_dates == True:\n",
    "    \n",
    "    #Warn us!\n",
    "    print('Dates files will be overwritten.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load in the dynamic climate indices file. If we are creating extreme train and test sets, this file is used to determine the extreme years for the desired climate variables, regardless of the input type specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pickled dynamic climate indices file\n",
    "with open(climate_indices_file, 'rb') as f:\n",
    "    climate_indices = pkl.load(f)\n",
    "        \n",
    "#If we are creating experiments for only the NWM years...\n",
    "if years == 'nwm': \n",
    "    \n",
    "    #For every basin...\n",
    "    for basin in basins:\n",
    "        \n",
    "        #Trim the datetime index to only include data within the NWM range\n",
    "        climate_indices[basin] = climate_indices[basin].loc[pd.date_range('1995-10-01','2014-12-30',freq='D')]\n",
    "\n",
    "#Check beginning time step\n",
    "print(\"Climate indices beginning at\",climate_indices[basins[0]].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that the number of basins from the configuration file match the number of basins avaliable in the climate_indices file, we run it through an assertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check that basins in the climate index match the basin set from the dummy config\n",
    "assert len(list(climate_indices.keys())) == len(basins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a peek at an example climate index for an example basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot an example climate index for an example basin\n",
    "plt.plot(climate_indices[basins[0]][use_climate_indexes[0]],c='teal')\n",
    "#Draw median line\n",
    "plt.axhline(y=np.nanmedian(climate_indices[basins[0]][use_climate_indexes[0]]),color='k',label='median')\n",
    "#Set xlabel\n",
    "plt.xlabel('Hydrological Years')\n",
    "#Set ylabel\n",
    "plt.ylabel(climate_indices[basins[0]][use_climate_indexes[0]].name)\n",
    "#Set title\n",
    "plt.title(f'Basin {basins[0]}: {climate_indices[basins[0]][use_climate_indexes[0]].name}')\n",
    "#Show legend\n",
    "plt.legend()\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If interactive plots enabled, must explicitly close plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps for creating the train andtest sets for the extreme climate experiments: we need to 1) define the train/test split function, 2) create the train and test sets using that function, and 3) save the train and test sets in the dates_dir (constituting the first external save of this notebook). \n",
    "\n",
    "But first, for fun, let's conceptualize how the extreme train and test sets are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of an example basin for an example index and sort based on index value\n",
    "ci_srt = climate_indices[basins[0]].dropna(0).sort_values(climate_indices[basins[0]].columns[0])\n",
    "\n",
    "#Create 'year' column by extracting the years from the dataframe's datetime index\n",
    "ci_srt['year'] = ci_srt.index.year\n",
    "#Extract the year column\n",
    "a = ci_srt.pop('year')\n",
    "#Re-insert the 'year column into the first column position'\n",
    "ci_srt.insert(value=a,column='year',loc=0)\n",
    "\n",
    "#Create a list of hydro years, sorted temporally\n",
    "xs = [int(x) for x in list(ci_srt.groupby('year').mean().index)]\n",
    "#Create a list of mean climate index values, sorted temporally\n",
    "ys = list(ci_srt.groupby('year').mean()[climate_indices[basins[0]].columns[0]])\n",
    "\n",
    "#Create a list of sorted years, sorted by mean climate index values\n",
    "sorted_years = list(ci_srt.groupby('year').mean().sort_values(climate_indices[basins[0]].columns[0]).index)  \n",
    "    \n",
    "#If we are creating extreme train/test sets...\n",
    "if exp_type == 'extreme':\n",
    "\n",
    "    #Create a list of sorted mean climate index values, sorted by mean climate index value\n",
    "    sorted_ys = list(ci_srt.groupby('year').mean().sort_values(climate_indices[basins[0]].columns[0])[climate_indices[basins[0]].columns[0]])\n",
    "\n",
    "    #Get a list of train years (one extreme)\n",
    "    train_years = sorted_years[:5]\n",
    "    #Get a list of train years' climate index values\n",
    "    train_ys = sorted_ys[:5]\n",
    "    #Get a list of test years (opposite extreme)\n",
    "    test_years = sorted_years[-5:]\n",
    "    #Get a list of test years' climate index values\n",
    "    test_ys = sorted_ys[-5:]\n",
    "    \n",
    "if exp_type == 'random':\n",
    "    \n",
    "    #Zip sorted_years and ys together to maintain year, value relationship during shuffle\n",
    "    temp = list(zip(sorted_years,ys))\n",
    "    #Shuffle them\n",
    "    np.random.shuffle(temp)\n",
    "    #Unzip them\n",
    "    random_years, random_ys = zip(*temp)\n",
    "    \n",
    "    #Get a list of train years\n",
    "    train_years = random_years[:5]\n",
    "    #Get a list of train years' climate index values\n",
    "    train_ys = random_ys[:5]\n",
    "    #Get a list of test years\n",
    "    test_years = random_years[-5:]\n",
    "    #Get a list of test years' climate index values\n",
    "    test_ys = random_ys[-5:]\n",
    "\n",
    "#Plot years and their corresponding climate index values\n",
    "plt.scatter(xs,ys,color='k')\n",
    "\n",
    "#Counters to limit setting labels once for each set\n",
    "itrain = 0\n",
    "itest = 0\n",
    "\n",
    "#For every x,y pair in xs (years) and ys (values)...\n",
    "for x,y in zip(xs,ys):\n",
    "    \n",
    "    #If y (value) is in train_ys...\n",
    "    if y in train_ys:\n",
    "        #Add 1 to itrain\n",
    "        itrain = itrain + 1\n",
    "        #And if itrain is equal to 1...\n",
    "        if itrain == 1:\n",
    "            \n",
    "            #Plot year and value as blue point and label\n",
    "            plt.scatter(x,y,color='blue',label='train years')\n",
    "            \n",
    "        #If itrain is not equal to 1...\n",
    "        else:\n",
    "            \n",
    "            #Plot year and value as blue point\n",
    "            plt.scatter(x,y,color='blue')\n",
    "            \n",
    "    #Or if y (value) is in test_years...\n",
    "    if y in test_ys:\n",
    "        \n",
    "        #Add 1 to itest\n",
    "        itest = itest + 1\n",
    "        #And if itest is equal to 1...\n",
    "        \n",
    "        if itest == 1:\n",
    "            \n",
    "            #Plot year and value as red point and label\n",
    "            plt.scatter(x,y,color='red',label='test years')\n",
    "            \n",
    "        #If itest is not equal to one...\n",
    "        else:\n",
    "            \n",
    "            #Plot year and value as red point\n",
    "            plt.scatter(x,y,color='red')\n",
    "\n",
    "#Set x-axis tick interval\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "#Draw line of median climate value\n",
    "plt.axhline(y=np.mean(climate_indices[basins[0]][climate_indices[basins[0]].columns[0]]),\n",
    "            color='k',lw=1,label='median')\n",
    "\n",
    "#Set x-axis label\n",
    "plt.xlabel('Hydrological Years')\n",
    "#Set y-axis label\n",
    "plt.ylabel(f'{climate_indices[basins[0]].columns[0]}')\n",
    "#Set title\n",
    "plt.title(f'Basin {basins[0]}: {exp_type.capitalize()} Train/Test Years')\n",
    "\n",
    "#Show legend\n",
    "plt.legend(loc=4)\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a function that orders data depending on the holdout type (\"high\" reverses the order of the argsorted list initialized later on), grabs the first n years for the test set and last n years for train set. It is important to understand this function, as it is the origin of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that splits train and test years based on climate values\n",
    "def train_test_split(idx, holdout, n_train, n_test):\n",
    "\n",
    "    #If the holdout type is high, reverse the argsort so that high index values are the test set (as seen below)\n",
    "    if holdout == 'high':\n",
    "        idx.reverse()\n",
    "        \n",
    "    #Grab the first few years as the test set\n",
    "    test = idx[:n_test]\n",
    "    \n",
    "    #Remove anything within sequence length of these years\n",
    "    for t in test:\n",
    "        try:\n",
    "            idx.remove(t)\n",
    "        except: \n",
    "            pass\n",
    "        try:\n",
    "            idx.remove(t+1)\n",
    "        except: \n",
    "            pass\n",
    "        try:\n",
    "            idx.remove(t-1)\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "    #Grab last few years as the train set\n",
    "    train = idx[-n_train:] \n",
    "    \n",
    "    #Return test and train sets\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we are creating experiments for either of the models...\n",
    "if nh or sacsma:\n",
    "    \n",
    "    #Initialize empty train/test dictionaries\n",
    "    test_dates = {experiment: {basin: {} for basin in basins} for experiment in experiments}\n",
    "    train_dates = {experiment: {basin: {} for basin in basins} for experiment in experiments}\n",
    "\n",
    "    #For n experiments...\n",
    "    for experiment in tqdm(experiments):\n",
    "        \n",
    "        #If we are creating extreme train/test sets\n",
    "        if exp_type == 'extreme':\n",
    "        \n",
    "            #Experiment name is split from \"_dyn\" and rejoin (in case we are creating a static experiment)\n",
    "            clim_index = \"_\".join(experiment.split('_')[:-1])+'_dyn'\n",
    "            htype = experiment.split('_')[-1]\n",
    "\n",
    "        #And for every basin...\n",
    "        for b, basin in enumerate(basins):\n",
    "            basin = str(basin)\n",
    "\n",
    "            #Retrieve the dates from the index of a basin's climate indices dataframe\n",
    "            dates = climate_indices[basin].index\n",
    "\n",
    "            #The following start/end dates correspond to hydrological years\n",
    "            #Extract any dates that are the 1st day of the 10th month \n",
    "            start_dates = climate_indices[basin].loc[(dates.month == 10) & (dates.day == 1)].index\n",
    "            #Extract any dates that are the 30th day of the 9th month\n",
    "            end_dates = climate_indices[basin].loc[(dates.month == 9) & (dates.day == 30)].index\n",
    "\n",
    "            #Exclude the first and last years\n",
    "            start_dates = start_dates[1:-1]\n",
    "            #Exclude first two years to ensure there is no incomplete data years\n",
    "            end_dates = end_dates[2:]\n",
    "\n",
    "            #If we are making random experiments...\n",
    "            if exp_type == 'random':\n",
    "\n",
    "                #Make a list of the start and end dates \"zipped\" together\n",
    "                temp = list(zip(start_dates, end_dates))\n",
    "                #Shuffle the start/end date pairs\n",
    "                np.random.shuffle(temp)\n",
    "                #Separate the pairs into start and end dates while keeping them in order\n",
    "                sorted_start_dates, sorted_end_dates = zip(*temp)\n",
    "\n",
    "                #Extract first 5 random start/end dates and store them for the corresponding experiment\n",
    "                test_dates[experiment][basin]['start_dates'] = list(sorted_start_dates[:5])\n",
    "                test_dates[experiment][basin]['end_dates'] = list(sorted_end_dates[:5])\n",
    "\n",
    "                #Extract last 5 random start/end dates and store them for the corresponding experiment\n",
    "                train_dates[experiment][basin]['start_dates'] = list(sorted_start_dates[-5:])\n",
    "                train_dates[experiment][basin]['end_dates'] = list(sorted_end_dates[-5:])\n",
    "\n",
    "            #If we are making extreme experiments...\n",
    "            if exp_type == 'extreme':\n",
    "\n",
    "                #Get index of all hydrological year end_dates and drop any dates that return a value of NaN\n",
    "                indexes_end_years = climate_indices[basin].loc[end_dates][clim_index].dropna()                \n",
    "\n",
    "                #Initialize list of hydrological start date indexes\n",
    "                indexes_start_years = []\n",
    "\n",
    "                #For every end date of an extreme year...\n",
    "                for ed in indexes_end_years.index:\n",
    "\n",
    "                    #Add the date exactly 364 days before the corresponding end_date to the start_years list\n",
    "                    indexes_start_years.append(ed + pd.to_timedelta(-364,unit='d'))\n",
    "\n",
    "                #Extract the numerical index of the start dates\n",
    "                indexes_start_years = climate_indices[basin].loc[indexes_start_years][clim_index]\n",
    "                \n",
    "                #Sort end_year indexes according to their corresponding climate index value\n",
    "                sorted_indexes_end_years = list(np.argsort(indexes_end_years.values))\n",
    "                #Make train and test index using train_test_split function\n",
    "                test_dex, train_dex = train_test_split(sorted_indexes_end_years, htype,\n",
    "                                                       n_train_years, n_test_years)\n",
    "\n",
    "                #Store test start & end dates for the corresponding experiment using the test_dex\n",
    "                test_dates[experiment][basin]['start_dates'] = list(np.sort(list(indexes_start_years.iloc[test_dex].index)))\n",
    "                test_dates[experiment][basin]['end_dates'] = list(np.sort(list(indexes_end_years.iloc[test_dex].index)))\n",
    "\n",
    "                #Store train start & end dates for the corresponding experiment using the train_dex\n",
    "                train_dates[experiment][basin]['start_dates'] = list(np.sort(list(indexes_start_years.iloc[train_dex].index)))\n",
    "                train_dates[experiment][basin]['end_dates'] = list(np.sort(list(indexes_end_years.iloc[train_dex].index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the SAC-SMA and NH libraries work with dictionaries of conflicting key-orders for the train and test sets. SAC-SMA is designed to read train and test set dictionaries with the format dict['start/end_dates'][basin] and NH is designed to read train and test set dictionaries with the format dict[basin]['start/end_dates']. This is accounted for below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If sacsma == True, we need to switch the order of the dictionary keys...\n",
    "if sacsma:\n",
    "    \n",
    "    #Initialize sacsma train & test dictionaries\n",
    "    sacsma_train_dates = {}\n",
    "    sacsma_test_dates = {}\n",
    "    \n",
    "    #For every experiment...\n",
    "    for experiment in list(train_dates.keys()):\n",
    "        \n",
    "        #Initialize an experiment level\n",
    "        sacsma_train_dates[experiment] = {}\n",
    "        sacsma_test_dates[experiment] = {}\n",
    "        \n",
    "        #For every start &end date...\n",
    "        for date in list(train_dates[experiment][basin].keys()):\n",
    "            \n",
    "            #Initalize a date level\n",
    "            sacsma_train_dates[experiment][date] = {}\n",
    "            sacsma_test_dates[experiment][date] = {}\n",
    "            \n",
    "            #And for every basin...\n",
    "            for basin in list(train_dates[experiment].keys()):\n",
    "                \n",
    "                #Input the dataframes made above into the sacsma train & test dictionaries\n",
    "                sacsma_train_dates[experiment][date][basin] = train_dates[experiment][basin][date]\n",
    "                sacsma_test_dates[experiment][date][basin] = test_dates[experiment][basin][date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test sets are saved as pickled dictionaries in the dates_dir defined above. Notice that these files are named dynamically and that the titles indicate the parameters of the experiment they will be used in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every experiment...\n",
    "for e, experiment in enumerate(tqdm(train_dates.keys())):\n",
    "    \n",
    "    #If we are creating train/test sets for neuralhydology...\n",
    "    if nh == True:\n",
    "\n",
    "        #If the date file is NOT already in dates_dir folder or if we want to overwrite that file...\n",
    "        if not f'nh_train_{exp_type}_{forcing}_{years}_{experiment}.pkl' in date_files or overwrite_dates == True:\n",
    "\n",
    "            #Define name of train/test file according to experiment parameters\n",
    "            test_fname = Path(f'{dates_dir}/nh_test_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "            train_fname = Path(f'{dates_dir}/nh_train_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "\n",
    "            #Save test file for an experiment at test_fname\n",
    "            with open(test_fname, 'wb') as f:\n",
    "                pkl.dump(test_dates[experiment], f)\n",
    "\n",
    "            #Save train file for an experiment at train_fname\n",
    "            with open(train_fname, 'wb') as f:\n",
    "                pkl.dump(train_dates[experiment], f)\n",
    "        \n",
    "    #If we are creating train/test sets for SAC-SMA...\n",
    "    if sacsma == True:\n",
    "\n",
    "        #If the date file is NOT already in dates_dir folder or if we want to overwrite that file...\n",
    "        if not f'sacsma_train_{forcing}_{exp_type}_{years}_{experiment}.pkl' in date_files or overwrite_dates == True:\n",
    "\n",
    "            #Define name of train/test file according to experiment parameters\n",
    "            test_fname = Path(f'{dates_dir}/sacsma_test_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "            train_fname = Path(f'{dates_dir}/sacsma_train_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "\n",
    "            #Save test file for an experiment at test_fname\n",
    "            with open(test_fname, 'wb') as f:\n",
    "                pkl.dump(sacsma_test_dates[experiment], f)\n",
    "\n",
    "            #Save train file for an experiment at train_fname\n",
    "            with open(train_fname, 'wb') as f:\n",
    "                pkl.dump(sacsma_train_dates[experiment], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the extreme/random train/test sets have been created and saved, we can actually CREATE the configuration files by editing the dummy configuration file and replacing dummy variables with the new, defined variables and save the configuration file to the into the run_config_dir defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we are creating config files for neuralhydrology...\n",
    "if nh:\n",
    "    \n",
    "    #If the path to the configuration files already exists or if we want to overwrite those configuration files...\n",
    "    if not os.path.isdir(nh_configs_path) or overwrite_configs == True:\n",
    "        \n",
    "        #For every experiment...\n",
    "        for e,experiment in enumerate(train_dates.keys()):\n",
    "\n",
    "            #For every seed...\n",
    "            for seed in seeds:\n",
    "\n",
    "                #Read in dummy config file\n",
    "                with open(dummy_config_file, 'r') as file :\n",
    "                    filedata = file.read()\n",
    "\n",
    "                #Change dummy variables...\n",
    "                #Experiment name\n",
    "                exp_name = f'nh_{inputs}_{exp_type}_{forcing}_{years}_{experiment}_{seed}'\n",
    "                #Location of run directory\n",
    "                run_dir = f'{working_dir}/nh_lstm/runs/{inputs}/{exp_type}/{forcing}/{years}'     \n",
    "\n",
    "                #Dummy train dates file\n",
    "                train_dates_fname = str(dates_dir / f'nh_train_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "                #Dummy test dates file\n",
    "                test_dates_fname = str(dates_dir / f'nh_test_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "\n",
    "                #Update dummy parameters \n",
    "                filedata = filedata.replace('dummy_run', exp_name)\n",
    "                filedata = filedata.replace('dummy_dir', run_dir)\n",
    "                filedata = filedata.replace('dummy_train_dates_file', train_dates_fname)\n",
    "                filedata = filedata.replace('dummy_test_dates_file', test_dates_fname)\n",
    "                filedata = filedata.replace('dummy_seed', f'{seed}')\n",
    "                filedata = filedata.replace('dummy_forcing', f'{forcing}')\n",
    "                filedata = filedata.replace('- dummy_dyn',f'- {ndyns[0]}\\n- {ndyns[1]}\\n- {ndyns[2]}\\n- {ndyns[3]}\\n- {ndyns[4]}')\n",
    "\n",
    "                #Write new config file to the pre-defined config_dir\n",
    "                new_config_path = Path(f'{nh_configs_path}')\n",
    "                new_config_file = new_config_path / f'{exp_name}.yml'\n",
    "\n",
    "                #If the path already exists, write files to it\n",
    "                if os.path.exists(new_config_path):\n",
    "                    with open(new_config_file, 'w') as file:\n",
    "                        file.write(filedata)\n",
    "\n",
    "                #If the path does not exist, create it and write files to it\n",
    "                else:\n",
    "                    os.makedirs(os.path.join(run_configs_dir,inputs,exp_type,forcing,years))\n",
    "                    with open(new_config_file, 'w') as file:\n",
    "                        file.write(filedata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we are creating config files for neuralhydrology...\n",
    "if sacsma:\n",
    "    \n",
    "    #If the path to the configuration files already exists or if we want to overwrite those configuration files...\n",
    "    if not os.path.isdir(sacsma_configs_path) or overwrite_configs == True:\n",
    "    \n",
    "        #For every experiment...\n",
    "        for e,experiment in enumerate(train_dates.keys()):\n",
    "\n",
    "            #For every seed...\n",
    "            for seed in seeds:\n",
    "\n",
    "                #Read in dummy config file\n",
    "                with open(dummy_config_file, 'r') as file :\n",
    "                    filedata = file.read()\n",
    "\n",
    "                #Change dummy variables...\n",
    "                #Experiment name\n",
    "                exp_name = f'sacsma_{inputs}_{exp_type}_{forcing}_{years}_{experiment}_{seed}'\n",
    "                #Location of run directory\n",
    "                run_dir = f'{working_dir}/sacsma/results'  \n",
    "                \n",
    "                #Dummy train dates file\n",
    "                train_dates_fname = str(dates_dir / f'sacsma_train_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "                #Dummy test dates file\n",
    "                test_dates_fname = str(dates_dir / f'sacsma_test_{exp_type}_{forcing}_{years}_{experiment}.pkl')\n",
    "\n",
    "                #Update dummy parameters \n",
    "                filedata = filedata.replace('dummy_run', exp_name)\n",
    "                filedata = filedata.replace('dummy_dir',run_dir)\n",
    "                filedata = filedata.replace('dummy_train_dates_file', train_dates_fname)\n",
    "                filedata = filedata.replace('dummy_test_dates_file', test_dates_fname)\n",
    "                filedata = filedata.replace('dummy_seed', f'{seed}')\n",
    "                filedata = filedata.replace('dummy_forcing', f'{forcing}')\n",
    "                filedata = filedata.replace('- dummy_dyn',f'- {sdyns[0]}\\n- {sdyns[1]}\\n- {sdyns[2]}\\n- {sdyns[3]}\\n- {sdyns[4]}')\n",
    "\n",
    "                #Write new config file to the pre-defined config_dir\n",
    "                new_config_path = Path(f'{sacsma_configs_path}')\n",
    "                new_config_file = new_config_path / f'{exp_name}.yml'\n",
    "\n",
    "                #If the path already exists, write files to it\n",
    "                if os.path.exists(new_config_path):\n",
    "                    with open(new_config_file, 'w') as file:\n",
    "                        file.write(filedata)\n",
    "                        \n",
    "                #If the path does not exist, create it and write files to it\n",
    "                else:\n",
    "                    os.makedirs(os.path.join(run_configs_dir,'sacsma/results'))\n",
    "                    with open(new_config_file, 'w') as file:\n",
    "                        file.write(filedata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have configuration files ready to be run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
