{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Job Scripts for SAC-SMA Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 2/X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: NEED SACSMA ENVIRONMENT ACTIVATED TO SEND OFF RUNS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In notebook 1/X we created configuration files for both Kratzert's NeuralHydrology's (NH) Long Short-Term Memory model and Nearing's SACSMA-SNOW17 (SAC-SMA) model. In this notebook, we create slurm job scripts pointing to the configuration files _specifically_ made for the SAC-SMA model and send them off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically reload modules; ensures most recent versions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the create_climate_experiments notebook (1/X), the most important parameters need to be defined first. As a reminder, 'inputs' refers to the nature of the inputs (static or dynamic), 'exp_type' refers to the experiment type (extreme or random), 'forcing' refers to the source of forcing data, and 'years' refers to whether we want to use all years or only years avaliable for the National Water Model as well.\n",
    "\n",
    "Defining these parameters directs the notebook to the corresponding folder containing the configuration files created in the first notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most Important Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#If files already exist, should they be overwritten by this notebook?\n",
    "overwrite = False\n",
    "\n",
    "#Explicitly define if you want to send off the created/defined runs\n",
    "send_runs = False\n",
    "\n",
    "#Specify experiment type; options include 'extreme' and 'random' (random experiments often used as benchmark)\n",
    "exp_type = 'extreme'\n",
    "\n",
    "#Specify ONE forcing data source; 5 options, including daymet, nldas(_extended), maurer(_extended)\n",
    "forcing = \"daymet\"\n",
    "\n",
    "#Specify years to use for experiments; 'all' or 'nwm'\n",
    "years = 'nwm'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just like the first notebook, additional experiment parameters are defined below. The SAC-SMA models are trained and tested on a CPU node so run characteristics need to be specified, including the maximum amount of model runs, amount of DDS trials, how much of the node to use, and which algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "max_model_runs = 1e4\n",
    "dds_trials = 1\n",
    "use_cores_frac = .90\n",
    "algorithm = 'DDS'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "\n",
    "#Path to orking directory\n",
    "working_dir = Path(os.getcwd()) \n",
    "\n",
    "#Path to run_configs directory (../configs/run_configs)\n",
    "run_configs_dir = working_dir / 'nh_lstm' / 'configs' / 'run_configs' \n",
    "\n",
    "#Path to job_scripts directory (../sacsma/job_scripts)\n",
    "job_scripts_path = working_dir / 'sacsma' / 'job_scripts'\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not have to edit anything below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to overwrite the job scripts, that's fine, but...\n",
    "if overwrite == True:\n",
    "    #Warn us!\n",
    "    print('\\033[91m'+'\\033[1m'+'Job scripts to be overwritten.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source data for this notebook consists of the SAC-SMA experiment configuration files we want to make slurm jobs for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC-SMA experiments only created with dynamic input tags for structure directory consistency;\n",
    "#SAC-SMA does not accept static or dynamic inputs\n",
    "inputs = 'dynamic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 experiments.\n"
     ]
    }
   ],
   "source": [
    "#Navigate to SAC-SMA experiment directory\n",
    "exp_config_dir = run_configs_dir / 'sacsma' / inputs / exp_type / forcing / years\n",
    "\n",
    "#Glob to get list of files (ending in .yml) in exp_config_dir\n",
    "config_files = list(exp_config_dir.glob('*.yml'))\n",
    "\n",
    "#Print number of experiment configurations in exp_config_dir\n",
    "print(f'There are {len(config_files)} experiments.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save Slurm Job Script Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through all of the configuration files in config_files, reference the \"dummy\", or representative, slurm file and replace its dummy variables. The dummy variables include experiment name, path to configuration file, and the optimizer hyperparameters defined above. Note that the experiment name is sourced from the configuration file name and that the job file is saved to the job_scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If job scripts for this experiment have not yet been made or if we want to overwrite them...\n",
    "if len(list(job_scripts_path.glob(f'**/sacsma_{exp_type}_{forcing}_{years}*.slurm'))) == 0 or overwrite == 'True':\n",
    "\n",
    "    #For every config file...\n",
    "    for c, config_file in enumerate(config_files):\n",
    "\n",
    "        #Open dummy slurm file\n",
    "        with open('run_job.slurm', 'r') as file:\n",
    "            filedata = file.read()\n",
    "\n",
    "        #Extract experiment name from config file name\n",
    "        exp_name = str(config_file).split('/')[-1]\n",
    "\n",
    "        #Replace dummy experiment name\n",
    "        filedata = filedata.replace('dummy', f'{exp_name}')\n",
    "\n",
    "        #Replace dummy config filepath\n",
    "        filedata = filedata.replace('${1}', str(config_file))\n",
    "\n",
    "        #Replace dummy max model runs\n",
    "        filedata = filedata.replace('${2}', str(int(max_model_runs)))\n",
    "\n",
    "        #Replace dummy algorithm\n",
    "        filedata = filedata.replace('${3}', algorithm)\n",
    "\n",
    "        #Extract config name\n",
    "        conf = str(config_file).split('/')[-1].split('.')[0]\n",
    "\n",
    "        #Define path to experiment job script file and save\n",
    "        job_file = Path(f'job_scripts/{conf}.slurm')\n",
    "        \n",
    "        #Write the job file\n",
    "        with open(job_file, 'w') as file:\n",
    "            file.write(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check, save a list of the experiment slurm files in job_scripts and print how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 experiments.\n"
     ]
    }
   ],
   "source": [
    "#Make list of slurm files in job_files folder for defined experiment\n",
    "job_files = list(Path('job_scripts').glob(f'**/sacsma_{inputs}_{exp_type}_{forcing}_{years}_*.slurm'))\n",
    "\n",
    "#Print number of job_files\n",
    "print(f'There are {len(job_files)} experiments.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now a job_scripts folder of runs ready to be run... but be warned (refer below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SAC-SMA Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WARNING: BE CAREFUL WHEN RUNNING NEXT CELL\n",
    "\n",
    "The next cell is a loop that loops through the corresponding experiment files in the job_script directory and sends them to be run on the CPU node, so you better be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we defined that we wanted runs to be sent off to the cpu...\n",
    "if send_runs:\n",
    "\n",
    "    #For every slurm file...\n",
    "    for file in job_files:\n",
    "\n",
    "        #Define run command\n",
    "        run_cmd = f\"sbatch {file}\"\n",
    "\n",
    "        #Define path to write log file to\n",
    "        log_file = f\"log_files/{str(file).split('/')[-1].split('.')[0]}\"\n",
    "\n",
    "        #Execute command and send off runs\n",
    "        with open(Path(log_file), 'w') as f:\n",
    "            subprocess.Popen(run_cmd, stderr=subprocess.STDOUT, stdout=f, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
